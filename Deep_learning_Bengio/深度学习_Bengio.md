#深度学习
##Bengio

#目录
##本书网站
##声明
##数学记号
##1.简介
	1.1谁适合看这本书
	1.2深度学习的历史趋势
##I 相关的数学和机器学习基础
##2.线性代数
	2.1标量，向量，矩阵，张量
	2.2矩阵和向量乘法
	2.3单位矩阵和逆矩阵
	2.4线性依赖和扩充
	2.5正规
	2.6特殊的矩阵和向量
	2.7特征分解Eigendecomposition
	2.8奇异值分解SVD
	2.9广义逆，伪逆
	2.10迹算子 Trace Operator
	2.11行列式 Determinant
	2.12例子：主成分分析 PCA
	
##3.概率论和信息论
	3.1为什么要说概率论
	3.2随机变量
	3.3概率分布
	3.4边际概率
	3.5条件概率
	3.6条件概率的链式法则
	3.7独立和条件独立
	3.8期望，方差和协方差
	3.9常用概率分布
	3.10一些有意义的特性
	3.11贝叶斯
	3.12连续变量的一些细致分析
	3.13信息论
	3.14结构化的概率模型
	
##4.数值计算
	4.1上溢和下溢
	4.2一些糟糕的情况
	4.3基于梯度的优化
	4.4约束优化
	4.5例子：线性最小序列
	
##5.机器学习基础
	5.1学习算法
	5.2能力，过拟合，欠拟合
	5.3超参数和验证集
	5.4估计，偏差和方差
	5.5最大似然估计
	5.6贝叶斯统计
	5.7监督学习
	5.8无监督学习
	5.9随机梯度下降
	5.10建立一个机器学习算法
	5.11被挑战所驱动的深度学习
	
##II 深度网络：现代实现

##6.深度前馈网络
	6.1例子：学习异或
	6.2基于梯度的学习
	6.3隐藏单元
	6.4框架设计
	6.5反向传播以及其他的算法
	6.6历史遗留
	
##7.深度学习的正则化
	7.1参数的正则惩罚项
	7.2正则惩罚项和约束优化
	7.3正则化和无约束问题
	7.4关于数据集的讨论
	7.5噪音，鲁棒性
	7.6半监督学习
	7.7多任务学习
	7.8Early Stop
	7.9参数绑定和参数共享
	7.10稀疏表示 Sparse Representations
	7.11Bagging和其他的集成方法
	7.12Dropout
	7.13对抗训练
	7.14切线距离，Tangent Prop和Manifold Tangent Classifier
	
##8.训练深度模型的优化
	8.1如何从纯优化问题中学习差异
	8.2神经网络中的的挑战
	8.3基本算法
	8.4参数初始化策略
	8.5适应学习速度的算法
	8.6估计的二级方法
	8.7优化策略和启发式算法
	
##9.卷积网络
	9.1卷积操作
	9.2动力
	9.3池化
	9.4卷积，池化和强烈的先验
	9.5基本卷积方程的变化
	9.6结构化的输出
	9.7数据类型
	9.8有效的卷积算法
	9.9随机或者无监督的特征
	9.10卷积网络的神经系统学基础
	9.11卷积网络和深度学习的历史
	
##10.序列化模型：递归和循环神经网络
	10.1计算图的演变
	10.2循环神经网络
	10.3双向RNN
	10.4Encoder-Decoder 序列-序列结构
	10.5深度循环神经网络
	10.6递归神经网络
	10.7长期依赖的挑战
	10.8反射状态网络
	10.9泄露单元(Leaky Unit)和一些别的多时序策略
	10.10长短期记忆模型和RNN的其他的门
	10.11长期依赖的优化
	10.12明确的记忆
	
##11.实用的方法
	11.1效果的衡量
	11.2默认的基准模型
	11.3决定是否需要收集更多数据
	11.4选择超参数
	11.5debug的策略
	11.6例子：多位数字识别
	
##12.应用
	12.1大规模深度学习
	12.2计算视觉
	12.3演讲识别
	12.4自然语言处理
	12.5其他应用

##III 深度学习的研究

##13.线性因子模型
	13.1概率论的主成分分析和因子分析
	13.2独立成分分析（ICA）
	13.3慢速特征分析
	13.4系数编码
	13.5PCA的多种解释
	
##14.自动编码
	14.1完全自动编码
	14.2正规化自动编码
	14.3Representational power，Layer Size and Depth
	14.4随机编码和解码
	14.5降噪自动编码
	14.6自动编码的学习流
	14.7弹性自动编码
	14.8预测系数分解
	14.9自动编码的应用
	
##15.表示学习
	15.1贪婪的层级预训练
	15.2迁移学习和领域适应
	15.3半监督的因果消解
	15.4分布式表示
	15.5从深度的指数级收益
	15.6一些发现潜在原因的线索

##16.深度学习的结构化概率模型
	16.1非结构化模型的挑战
	16.2使用图描述模型结构
	16.3图模型的采样
	16.4结构化模型的优势
	16.5学习的依赖
	16.6接口和临界界面
	16.7深度学习方法结构化概率模型
	
##17.蒙特卡洛方法
	17.1采样和蒙特卡洛方法
	17.2主要性采样
	17.3马尔科夫链，蒙特卡洛方法
	17.4吉布斯采样
	17.5两种方式混合的挑战
	
##18.配分函数的对立
	18.1对数似然梯度
	18.2随机极大似然和对比分歧
	18.3拟似然估计
	18.4得分匹配和比率匹配
	18.5降噪分数匹配
	18.6噪声对比估计
	18.7估计配分函数

##19.临界界面
	19.1优化的接口
	19.2最大期望
	19.3MAP接口和稀疏编码
	19.4多种接口和学习
	19.5学习临界界面
	
##20.深度生成模型
	20.1玻尔兹曼机
	20.2严格的玻尔兹曼机
	20.3深度置信网络
	20.4深度玻尔兹曼机
	20.5实值数据的玻尔兹曼机
	20.6卷机玻尔兹曼机
	20.7结构化或序列化输出的玻尔兹曼机
	20.8其他的玻尔兹曼机
	20.9随机操作的反向传播
	20.10有向生成网络
	20.11自动编码的采样
	20.12生成随机网络
	20.13其他的生成模式
	20.14评估生成模型
	20.15结论

##参考文献

##索引