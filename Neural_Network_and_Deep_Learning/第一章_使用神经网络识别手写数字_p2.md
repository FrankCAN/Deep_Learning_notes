## 梯度下降

现在，我们有了一个神经网络的设计，那么这个神经网络是如何学习识别图像中的数字呢？第一件事情，我们需要一个数据集来进行我们的实验，使用这个数据集来训练我们的神经网络，让他能够学习去识别手写数字，这个数据集被称为训练集。我们将使用MNIST数据集，它包含有上万个手写数字的扫面图像，MNIST这个名字的由来是因为他是两个NIST提供的数据集的修改的子集，NIST是 united states national institute of standards and technology.下面是一些MNIST的样本：
![](./img/Ch1/Ch1.fig15.png)

你可能会发现了，这些数字实际上就是我们在上文中展示的数据。不过请放心，在最后我们用来测试神经网络的时候绝对不会使用和训练数据有重叠的数据的。

MNIST数据由两部分组成，第一部分包含有60000个图片是训练集。这些图片是扫描了250个人的手写数字，其中，一半的人在US Census Bureau工作，另一半是高中生。这些图片都是28\*28个像素点的灰度图片。第二部分是10000个测试图片，同样，也是28*28像素的灰度图片。我们会使用测试集来验证我们的神经网络的对于手写数字是别的好坏。为了对神经网络的表现做出一个好的测试，测试数据的来源是250个和写训练数据不一样的人（不过还是两组人一组是census bureau的员工还有高中生）。这可以让我们相信我们的系统可以识别出手写数字，而不是仅仅学会了识别这几个人写的数字。

我们将使用x来标记训练的输入数据，把每一个训练数据的输入x看做一个28*28=784维的向量是很方便的。向量中的每一项都代表图像中的每一个像素点的灰度。我们将期望的正确的输出标记为$y=y(x)，y$是一个10维的向量。举个栗子对于一个特定的训练图像，x，画了一个6，那么输出$y(x)=(0,0,0,0,0,0,1,0,0,0)^T$就是我们期望从这个网络得到的输出。其中的T表示转置，就是把一个行向量变成一个列向量。

我们所期望的是有一个算法可以让我们找到合适的权重和偏移量，使得网络对于每一个训练输入x都可以有近似于y(x)的输出。为了衡量我们的网络的效果（这里就是识别手写数字的准确程度），我们定义了一个代价函数（有些时候会被称为缺失函数或者公正函数，我们在本书中称其为代价函数，但是你需要知道别人不一定这么叫，因为这个东西经常在一些paper中出现，但是名字可能不尽相同）：
$\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\tag{6}\end{eqnarray}$

其中，w表示网络中所有权重的集合，b是所有的偏移量，n是训练集的个数，a是当网络中的输入为x时的输出向量，求和是在整个训练集的数据域上。当然了，我们的输出a，依赖于x，w和b，但是为了让我们的描述更加的简洁，在这里我们就不明确的写出其中的依赖关系了。符号$\| v \|$表示一个向量v的模。我们把C称为二次代价函数，或者我们会称之为均方误差（mean squared error）或者缩写成MSE。我们再来看一下这个代价函数的函数形式就会发现，c(w,b)是一个非负的值，因为，其中的每一项——求和的每一项||y(x)-a||的平方一定是一个非负的值。而且，当对于每个输入x的输出值a，越来越接近于期望值y(x)的时候我们的代价函数会逼近于0。所以如果我们的学习算法可以通过训练来学习权值和偏移量，使得C(w,b)近乎于零，那我们的训练算法就是一个好的算法。从另一个方面来讲，如果我们的C(w,b)的值很大，那么我们的训练算法就不是很好，也就意味着a和我们对于输入x的期望输出y(x)相差很大。这时我们训练模型的算法就是通过调整权重和偏移量来减小代价函数C(w,b)的值。换句话说，我们想要找到一个权重和偏移量的集合，使得代价函数的值尽可能的小，我们将通过梯度下降算法来完成我们的目标。

现在，假设我们在尝试最小化代价函数，$C(v)$。我们假设，我们的函数可以是任意的实数域上的多元函数$v=v1,v2,v3,.....$要注意，这里，我们把w,b用v来表示，再次强调，这个函数可以是任意形式的，在这里，我们忽略掉神经网络这个上下文。为了得到最小的C(v)，把函数C假设成一个只有两个变量v1,v2的函数，然后画出来他的图
像，来帮助我们理解。
![](./img/Ch1/Ch1.fig20.png)

我们想要找到，一个点C在这个点可以得到全局最小值。当然了，对于上面那个图像而言，我们用眼就能找到最小值。仅凭直觉，我们就可以找到一个简单函数的最小值。但是，对于一个函数C,可能具有复杂的函数形式，有很多的变量，直接找到最小值，就是不可能的了。

解决这个问题的一个方法就是使用微积分来分析找到最小值。我们可以通过计算偏导，然后试着使用偏导来求出C在什么地方可以达到极值。当C的变量个数不多的时候，多数情况下，我们还是可以做到这一点的，但是，当我们有多个变量的时候这就变成了一场，噩梦。并且，对于神经网络，我们总是希可以有更多的变量——那些大的神经网络的代价函数，依赖于十亿多组权重和偏移量。微积分绝对是不适用的。

（目前为止，我们只讨论了拥有两个变量的代价函数，但是，让我们回过头来，到上面两段的时候，说“hey，如果我们的代价函数有多于两个变量呢？”对于这种情况，我们只能说，很抱歉。 不过，请相信，当我说想象C，这个代价函数只有两个参数，这对于我们的理解是很有帮助的。 It just happens that sometimes that picture breaksdown, and the last two paragraphs were dealing with such breakdowns。（当函数图像发生断点，并且两个子图在断点附近的时候，使用微积分变得不是很好分析了。） 使用函数图像对于我们在数学上的思考是很有帮助的，我们要学会找到最合适使用图像来理解的地方。）

好了，现在我们会发现，使用微积分好像不太好用。幸运的是，我们还有别的数学工具，而且，在这个问题上，我们的新方法是很有效的。首先，我们来想象一下，将我们的函数当做一个山谷。如果你觉得有点怪，看一眼上面的函数图像。然后，我们假设有个球，从山谷的斜坡上滚下来了。我们的生活经验会告诉我们，那个球会一路滚到山谷的底部。那么我们是不是可以模仿这种行为来找到函数的最小值呢。我们首先在图像上随机找一个起点（在滚球的时候我们假设是随机找个位置放球），然后模拟球滚动到谷底的行为。我们可移动过计算代价函数的偏导来简单地模拟这种行为（有些情况下我们需要二阶偏导），这些偏导可以提供我们找到局部“尖点”的条件。然后我们的球就可以开始向下滚动了。

根据我们刚刚说的那些，你可能会认为接下来就应该通过牛顿方程来求解球的行为——计算重力啊，摩擦力之类的。
但是，实际上，我们不会那么的，，严肃，我们的算法仅仅是去找到代价函数的最小值，而不是构建一个严格的遵
循物理定律的世界。我们需要站在球的角度上来思考整个问题，而不是我们人的角度。所以，我们不要去考虑那些
复杂的，甚至有点混乱的物理世界，让我们以一种更加简单的角度来看整个事情：我们构建一个单纯的世界，自己
来定义物理规则，规定这个球要怎么去滚到底部，那么，我们会选择怎样的行为，规则，来保证，这个球，会始终
到达底部。
