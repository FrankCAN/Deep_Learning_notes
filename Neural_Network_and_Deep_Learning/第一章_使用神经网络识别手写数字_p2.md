## 梯度下降

现在，我们有了一个神经网络的设计，那么这个神经网络是如何学习识别图像中的数字呢？第一件事情，我们需要一个数据集来进行我们的实验，使用这个数据集来训练我们的神经网络，让他能够学习去识别手写数字，这个数据集被称为训练集。我们将使用MNIST数据集，它包含有上万个手写数字的扫面图像，MNIST这个名字的由来是因为他是两个NIST提供的数据集的修改的子集，NIST是 united states national institute of standards and technology.下面是一些MNIST的样本：
![](./img/Ch1/Ch1.fig15.png)

你可能会发现了，这些数字实际上就是我们在上文中展示的数据。不过请放心，在最后我们用来测试神经网络的时候绝对不会使用和训练数据有重叠的数据的。

MNIST数据由两部分组成，第一部分包含有60000个图片是训练集。这些图片是扫描了250个人的手写数字，其中，一半的人在US Census Bureau工作，另一半是高中生。这些图片都是28\*28个像素点的灰度图片。第二部分是10000个测试图片，同样，也是28*28像素的灰度图片。我们会使用测试集来验证我们的神经网络的对于手写数字是别的好坏。为了对神经网络的表现做出一个好的测试，测试数据的来源是250个和写训练数据不一样的人（不过还是两组人一组是census bureau的员工还有高中生）。这可以让我们相信我们的系统可以识别出手写数字，而不是仅仅学会了识别这几个人写的数字。

我们将使用x来标记训练的输入数据，把每一个训练数据的输入x看做一个28*28=784维的向量是很方便的。向量中的每一项都代表图像中的每一个像素点的灰度。我们将期望的正确的输出标记为$y=y(x)，y$是一个10维的向量。举个栗子对于一个特定的训练图像，x，画了一个6，那么输出$y(x)=(0,0,0,0,0,0,1,0,0,0)^T$就是我们期望从这个网络得到的输出。其中的T表示转置，就是把一个行向量变成一个列向量。

我们所期望的是有一个算法可以让我们找到合适的权重和偏移量，使得网络对于每一个训练输入x都可以有近似于y(x)的输出。为了衡量我们的网络的效果（这里就是识别手写数字的准确程度），我们定义了一个代价函数（有些时候会被称为缺失函数或者公正函数，我们在本书中称其为代价函数，但是你需要知道别人不一定这么叫，因为这个东西经常在一些paper中出现，但是名字可能不尽相同）：
$\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\tag{6}\end{eqnarray}$

其中，w表示网络中所有权重的集合，b是所有的偏移量，n是训练集的个数，a是当网络中的输入为x时的输出向量，求和是在整个训练集的数据域上。当然了，我们的输出a，依赖于x，w和b，但是为了让我们的描述更加的简洁，在这里我们就不明确的写出其中的依赖关系了。符号$\| v \|$表示一个向量v的模。我们把C称为二次代价函数，或者我
们会称之为均方差（mean squared error）或者缩写成MSE。我们再来看一下这个代价函数的函数形式就会发现，
c(w,b)是一个非负的值，因为，其中的每一项——求和的每一项||y(x)-a||的平方一定是一个非负的值。而且，当输出
值a，对于每个输入x，越来越接近于期望值y(x)的时候我们的代价函数会逼近于0。所以如果我们的学习算法可以通过
训练来学习权值和偏移量，使得C(w,b)近乎于零，那我们的训练算法就是一个好的算法。从另一个方面来讲，如果
我们的C(w,b)的值很大，那么我们的训练算法就不是很好，也就意味着a和我们对于输入x的期望输出y(x)相差很大。
这时我们训练算法的模型就是通过调整权重和偏移量来减小代价函数C(w,b)的值。换句话说，我们想要找到一个权重
和偏移量的集合，可以使得代价函数的值尽可能的小，我们将使用梯度下降算法来完成我们的目标。
